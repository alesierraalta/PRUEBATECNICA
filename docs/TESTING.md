# Gu√≠a de Pruebas

## Resumen

Esta gu√≠a detalla la estrategia de pruebas integral para el Microservicio de Resumen LLM, incluyendo pruebas unitarias, de integraci√≥n, de rendimiento y de seguridad. El proyecto mantiene una cobertura de pruebas del 80%+ y utiliza herramientas modernas de testing.

## Estructura de Pruebas

```
tests/
‚îú‚îÄ‚îÄ conftest.py              # Fixtures globales y configuraci√≥n
‚îú‚îÄ‚îÄ fixtures/
‚îÇ   ‚îî‚îÄ‚îÄ test_data.py         # Datos de prueba reutilizables
‚îú‚îÄ‚îÄ unit/                    # Pruebas unitarias
‚îÇ   ‚îú‚îÄ‚îÄ test_config.py       # Pruebas de configuraci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ test_schemas.py      # Pruebas de esquemas Pydantic
‚îÇ   ‚îú‚îÄ‚îÄ test_services.py     # Pruebas de servicios
‚îÇ   ‚îú‚îÄ‚îÄ test_providers.py    # Pruebas de proveedores
‚îÇ   ‚îî‚îÄ‚îÄ test_utils.py        # Pruebas de utilidades
‚îú‚îÄ‚îÄ integration/             # Pruebas de integraci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ test_api.py          # Pruebas de endpoints API
‚îÇ   ‚îú‚îÄ‚îÄ test_cache.py        # Pruebas de integraci√≥n con Redis
‚îÇ   ‚îî‚îÄ‚îÄ test_llm.py          # Pruebas de integraci√≥n con LLM
‚îú‚îÄ‚îÄ performance/             # Pruebas de rendimiento
‚îÇ   ‚îú‚îÄ‚îÄ test_load.py         # Pruebas de carga
‚îÇ   ‚îî‚îÄ‚îÄ test_stress.py       # Pruebas de estr√©s
‚îî‚îÄ‚îÄ security/               # Pruebas de seguridad
    ‚îú‚îÄ‚îÄ test_auth.py         # Pruebas de autenticaci√≥n
    ‚îî‚îÄ‚îÄ test_input.py        # Pruebas de validaci√≥n de entrada
```

## Configuraci√≥n de Pruebas

### pytest.ini

```ini
[tool:pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = 
    --strict-markers
    --strict-config
    --cov=app
    --cov-report=term-missing
    --cov-report=html
    --cov-report=xml
    --cov-fail-under=80
    --asyncio-mode=auto
    -v
markers =
    unit: Pruebas unitarias r√°pidas
    integration: Pruebas de integraci√≥n que requieren servicios externos
    slow: Pruebas que toman m√°s de 5 segundos
    performance: Pruebas de rendimiento
    security: Pruebas de seguridad
    llm: Pruebas que requieren acceso a LLM
    redis: Pruebas que requieren Redis
```

### conftest.py

```python
import asyncio
import pytest
from fastapi.testclient import TestClient
from unittest.mock import AsyncMock, MagicMock

from app.main import app
from app.config import settings
from app.providers.cache.redis_cache import CacheService
from app.providers.llm.gemini import GeminiProvider

@pytest.fixture(scope="session")
def event_loop():
    """Crear event loop para toda la sesi√≥n de pruebas."""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture
def client():
    """Cliente de prueba FastAPI."""
    return TestClient(app)

@pytest.fixture
def mock_cache_service():
    """Mock del servicio de cach√©."""
    cache = AsyncMock(spec=CacheService)
    cache.get.return_value = None
    cache.set.return_value = None
    cache.check_health.return_value = "Redis saludable"
    return cache

@pytest.fixture
def mock_gemini_provider():
    """Mock del proveedor Gemini."""
    provider = AsyncMock(spec=GeminiProvider)
    provider.summarize.return_value = {
        "summary": "Resumen de prueba generado",
        "usage": {"prompt_tokens": 100, "completion_tokens": 50, "total_tokens": 150},
        "model": "gemini-pro",
        "latency_ms": 1000.0
    }
    provider.check_health.return_value = "Gemini API saludable"
    return provider

@pytest.fixture
def sample_text():
    """Texto de muestra para pruebas."""
    return """
    La inteligencia artificial est√° revolucionando m√∫ltiples industrias en todo el mundo.
    Desde la atenci√≥n m√©dica hasta las finanzas, las tecnolog√≠as de IA est√°n permitiendo
    niveles sin precedentes de automatizaci√≥n y eficiencia. Los modelos de lenguaje
    grandes como GPT, Claude y Gemini est√°n transformando la forma en que interactuamos
    con la tecnolog√≠a y procesamos informaci√≥n.
    """

@pytest.fixture
def api_key():
    """Clave API de prueba."""
    return "test_api_key_123"
```

## Pruebas Unitarias

### Pruebas de Configuraci√≥n

```python
# tests/unit/test_config.py
import pytest
from pydantic import ValidationError

from app.config import settings

class TestConfig:
    def test_default_values(self):
        """Probar valores predeterminados de configuraci√≥n."""
        assert settings.LOG_LEVEL == "INFO"
        assert settings.SUMMARY_MAX_TOKENS == 100
        assert settings.REQUEST_TIMEOUT_MS == 10000

    def test_environment_variable_override(self, monkeypatch):
        """Probar que las variables de entorno sobrescriben valores predeterminados."""
        monkeypatch.setenv("LOG_LEVEL", "DEBUG")
        monkeypatch.setenv("SUMMARY_MAX_TOKENS", "200")
        
        # Recargar configuraci√≥n
        from app.config import Settings
        test_settings = Settings()
        
        assert test_settings.LOG_LEVEL == "DEBUG"
        assert test_settings.SUMMARY_MAX_TOKENS == 200

    def test_required_variables(self):
        """Probar que las variables requeridas est√°n presentes."""
        assert settings.API_KEYS_ALLOWED is not None
        assert len(settings.API_KEYS_ALLOWED.split(",")) > 0

    def test_invalid_log_level(self, monkeypatch):
        """Probar validaci√≥n de nivel de log inv√°lido."""
        monkeypatch.setenv("LOG_LEVEL", "INVALID")
        
        with pytest.raises(ValidationError):
            from app.config import Settings
            Settings()

    def test_cors_origins_parsing(self, monkeypatch):
        """Probar parsing de or√≠genes CORS."""
        monkeypatch.setenv("CORS_ORIGINS", "https://example.com,https://test.com")
        
        from app.config import Settings
        test_settings = Settings()
        
        assert "https://example.com" in test_settings.CORS_ORIGINS
        assert "https://test.com" in test_settings.CORS_ORIGINS
```

### Pruebas de Esquemas

```python
# tests/unit/test_schemas.py
import pytest
from pydantic import ValidationError

from app.api.schemas.summarize import SummarizeRequest, SummarizeResponse
from app.api.schemas.health import HealthResponse

class TestSummarizeRequest:
    def test_valid_request(self):
        """Probar solicitud v√°lida."""
        request = SummarizeRequest(
            text="Este es un texto de prueba para resumir.",
            lang="es",
            max_tokens=100,
            tone="neutral"
        )
        
        assert request.text == "Este es un texto de prueba para resumir."
        assert request.lang == "es"
        assert request.max_tokens == 100
        assert request.tone == "neutral"

    def test_default_values(self):
        """Probar valores predeterminados."""
        request = SummarizeRequest(text="Texto de prueba")
        
        assert request.lang == "auto"
        assert request.max_tokens == 100
        assert request.tone == "neutral"
        assert request.evaluate_quality == False

    def test_text_too_short(self):
        """Probar validaci√≥n de texto muy corto."""
        with pytest.raises(ValidationError) as exc_info:
            SummarizeRequest(text="Corto")
        
        errors = exc_info.value.errors()
        assert any(error["type"] == "string_too_short" for error in errors)

    def test_text_too_long(self):
        """Probar validaci√≥n de texto muy largo."""
        long_text = "a" * 50001  # Excede el l√≠mite de 50,000 caracteres
        
        with pytest.raises(ValidationError) as exc_info:
            SummarizeRequest(text=long_text)
        
        errors = exc_info.value.errors()
        assert any(error["type"] == "string_too_long" for error in errors)

    def test_invalid_language(self):
        """Probar validaci√≥n de idioma inv√°lido."""
        with pytest.raises(ValidationError) as exc_info:
            SummarizeRequest(text="Texto de prueba", lang="invalid")
        
        errors = exc_info.value.errors()
        assert any(error["type"] == "literal_error" for error in errors)

    def test_invalid_tone(self):
        """Probar validaci√≥n de tono inv√°lido."""
        with pytest.raises(ValidationError) as exc_info:
            SummarizeRequest(text="Texto de prueba", tone="invalid")
        
        errors = exc_info.value.errors()
        assert any(error["type"] == "literal_error" for error in errors)

    def test_max_tokens_out_of_range(self):
        """Probar validaci√≥n de max_tokens fuera de rango."""
        with pytest.raises(ValidationError) as exc_info:
            SummarizeRequest(text="Texto de prueba", max_tokens=5)  # M√≠nimo es 10
        
        errors = exc_info.value.errors()
        assert any(error["type"] == "greater_than_equal" for error in errors)

class TestSummarizeResponse:
    def test_valid_response(self):
        """Probar respuesta v√°lida."""
        response = SummarizeResponse(
            summary="Resumen generado",
            usage={"prompt_tokens": 100, "completion_tokens": 50, "total_tokens": 150},
            model="gemini-pro",
            latency_ms=1000.0,
            cached=False
        )
        
        assert response.summary == "Resumen generado"
        assert response.usage["total_tokens"] == 150
        assert response.model == "gemini-pro"
        assert response.latency_ms == 1000.0
        assert response.cached == False
```

### Pruebas de Servicios

```python
# tests/unit/test_services.py
import pytest
from unittest.mock import AsyncMock, patch

from app.services.cache import CacheService
from app.services.evaluation import SummaryEvaluator

class TestCacheService:
    @pytest.mark.asyncio
    async def test_get_cache_hit(self):
        """Probar acierto de cach√©."""
        cache_service = CacheService()
        
        # Mock Redis response
        with patch('aiocache.Cache.get') as mock_get:
            mock_get.return_value = {"summary": "Resumen cachead"}
            
            result = await cache_service.get("test_key")
            
            assert result == {"summary": "Resumen cachead"}
            mock_get.assert_called_once_with("test_key")

    @pytest.mark.asyncio
    async def test_get_cache_miss(self):
        """Probar fallo de cach√©."""
        cache_service = CacheService()
        
        with patch('aiocache.Cache.get') as mock_get:
            mock_get.return_value = None
            
            result = await cache_service.get("test_key")
            
            assert result is None

    @pytest.mark.asyncio
    async def test_set_cache(self):
        """Probar almacenamiento en cach√©."""
        cache_service = CacheService()
        
        with patch('aiocache.Cache.set') as mock_set:
            mock_set.return_value = True
            
            await cache_service.set("test_key", {"summary": "Resumen"}, 3600)
            
            mock_set.assert_called_once_with("test_key", {"summary": "Resumen"}, ttl=3600)

    @pytest.mark.asyncio
    async def test_cache_key_generation(self):
        """Probar generaci√≥n de claves de cach√©."""
        cache_service = CacheService()
        
        key1 = cache_service._generate_cache_key("texto1", "es", 100, "neutral")
        key2 = cache_service._generate_cache_key("texto1", "es", 100, "neutral")
        key3 = cache_service._generate_cache_key("texto2", "es", 100, "neutral")
        
        # Mismas entradas deben generar la misma clave
        assert key1 == key2
        
        # Diferentes entradas deben generar claves diferentes
        assert key1 != key3

class TestSummaryEvaluator:
    def test_rouge_calculation(self):
        """Probar c√°lculo de m√©tricas ROUGE."""
        evaluator = SummaryEvaluator()
        
        original = "El gato est√° en la casa. La casa es grande."
        summary = "El gato est√° en la casa grande."
        
        rouge_scores = evaluator._calculate_rouge_scores(original, summary)
        
        assert "rouge-1" in rouge_scores
        assert "rouge-2" in rouge_scores
        assert "rouge-l" in rouge_scores
        
        # Verificar que las puntuaciones est√°n en el rango [0, 1]
        for metric, score in rouge_scores.items():
            assert 0 <= score <= 1

    def test_semantic_similarity(self):
        """Probar c√°lculo de similitud sem√°ntica."""
        evaluator = SummaryEvaluator()
        
        text1 = "El gato est√° durmiendo en el sof√°"
        text2 = "Un felino descansa en el mueble"
        
        similarity = evaluator._calculate_semantic_similarity(text1, text2)
        
        assert 0 <= similarity <= 1
        assert similarity > 0.5  # Deber√≠a ser similar sem√°nticamente

    def test_compression_ratio(self):
        """Probar c√°lculo de ratio de compresi√≥n."""
        evaluator = SummaryEvaluator()
        
        original = "Este es un texto muy largo con muchas palabras."
        summary = "Texto largo."
        
        ratio = evaluator._calculate_compression_ratio(original, summary)
        
        assert 0 <= ratio <= 1
        assert ratio < 0.5  # El resumen deber√≠a ser m√°s corto

    @pytest.mark.asyncio
    async def test_evaluate_summary(self):
        """Probar evaluaci√≥n completa de resumen."""
        evaluator = SummaryEvaluator()
        
        original = "La inteligencia artificial est√° transformando m√∫ltiples industrias."
        summary = "La IA transforma industrias."
        
        evaluation = await evaluator.evaluate(original, summary)
        
        assert "rouge_1_f" in evaluation
        assert "rouge_2_f" in evaluation
        assert "rouge_l_f" in evaluation
        assert "semantic_similarity" in evaluation
        assert "compression_ratio" in evaluation
        assert "quality_score" in evaluation
```

## Pruebas de Integraci√≥n

### Pruebas de API

```python
# tests/integration/test_api.py
import pytest
from fastapi.testclient import TestClient
from unittest.mock import patch

from app.main import app

class TestSummarizeAPI:
    def test_summarize_endpoint_success(self, client, sample_text, api_key):
        """Probar endpoint de resumen exitoso."""
        with patch('app.providers.llm.gemini.GeminiProvider.summarize') as mock_summarize:
            mock_summarize.return_value = {
                "summary": "Resumen generado por IA",
                "usage": {"prompt_tokens": 100, "completion_tokens": 50, "total_tokens": 150},
                "model": "gemini-pro",
                "latency_ms": 1000.0
            }
            
            response = client.post(
                "/v1/summarize",
                headers={"Authorization": f"Bearer {api_key}"},
                json={
                    "text": sample_text,
                    "lang": "es",
                    "max_tokens": 100,
                    "tone": "neutral"
                }
            )
            
            assert response.status_code == 200
            data = response.json()
            assert "summary" in data
            assert "usage" in data
            assert "model" in data
            assert "latency_ms" in data

    def test_summarize_endpoint_unauthorized(self, client, sample_text):
        """Probar endpoint sin autenticaci√≥n."""
        response = client.post(
            "/v1/summarize",
            json={"text": sample_text}
        )
        
        assert response.status_code == 401
        assert "error" in response.json()

    def test_summarize_endpoint_invalid_api_key(self, client, sample_text):
        """Probar endpoint con clave API inv√°lida."""
        response = client.post(
            "/v1/summarize",
            headers={"Authorization": "Bearer invalid_key"},
            json={"text": sample_text}
        )
        
        assert response.status_code == 401

    def test_summarize_endpoint_validation_error(self, client, api_key):
        """Probar endpoint con datos de entrada inv√°lidos."""
        response = client.post(
            "/v1/summarize",
            headers={"Authorization": f"Bearer {api_key}"},
            json={
                "text": "Muy corto",  # Muy corto para el m√≠nimo
                "max_tokens": 5,      # Muy bajo para el m√≠nimo
                "lang": "invalid"     # Idioma inv√°lido
            }
        )
        
        assert response.status_code == 400
        data = response.json()
        assert "error" in data
        assert "details" in data

    def test_summarize_endpoint_rate_limit(self, client, sample_text, api_key):
        """Probar limitaci√≥n de velocidad."""
        # Hacer m√∫ltiples solicitudes r√°pidamente
        for _ in range(10):
            response = client.post(
                "/v1/summarize",
                headers={"Authorization": f"Bearer {api_key}"},
                json={"text": sample_text}
            )
        
        # La √∫ltima solicitud deber√≠a ser limitada por velocidad
        assert response.status_code == 429

class TestHealthAPI:
    def test_health_endpoint_success(self, client):
        """Probar endpoint de salud exitoso."""
        with patch('app.providers.llm.gemini.GeminiProvider.check_health') as mock_health:
            mock_health.return_value = "Gemini API saludable"
            
            response = client.get("/v1/healthz")
            
            assert response.status_code == 200
            data = response.json()
            assert data["overall_status"] in ["healthy", "degraded", "unhealthy"]
            assert "services" in data
            assert "timestamp" in data
            assert "version" in data

    def test_health_endpoint_no_auth_required(self, client):
        """Probar que el endpoint de salud no requiere autenticaci√≥n."""
        response = client.get("/v1/healthz")
        
        # No deber√≠a devolver 401
        assert response.status_code != 401
```

### Pruebas de Integraci√≥n con Redis

```python
# tests/integration/test_cache.py
import pytest
import asyncio
from app.services.cache import CacheService

@pytest.mark.integration
@pytest.mark.redis
class TestRedisIntegration:
    @pytest.fixture(scope="class")
    async def redis_service(self):
        """Servicio Redis real para pruebas de integraci√≥n."""
        service = CacheService()
        await service._initialize()
        yield service
        await service._cleanup()

    @pytest.mark.asyncio
    async def test_redis_connection(self, redis_service):
        """Probar conexi√≥n a Redis."""
        health_status = await redis_service.check_health()
        assert "saludable" in health_status.lower() or "healthy" in health_status.lower()

    @pytest.mark.asyncio
    async def test_cache_set_and_get(self, redis_service):
        """Probar almacenamiento y recuperaci√≥n de cach√©."""
        test_key = "test_integration_key"
        test_value = {"summary": "Resumen de prueba", "model": "test"}
        
        # Almacenar en cach√©
        await redis_service.set(test_key, test_value, 60)
        
        # Recuperar del cach√©
        result = await redis_service.get(test_key)
        
        assert result == test_value

    @pytest.mark.asyncio
    async def test_cache_expiration(self, redis_service):
        """Probar expiraci√≥n de cach√©."""
        test_key = "test_expiration_key"
        test_value = {"summary": "Resumen temporal"}
        
        # Almacenar con TTL muy corto
        await redis_service.set(test_key, test_value, 1)
        
        # Esperar a que expire
        await asyncio.sleep(2)
        
        # Intentar recuperar
        result = await redis_service.get(test_key)
        
        assert result is None

    @pytest.mark.asyncio
    async def test_cache_key_generation(self, redis_service):
        """Probar generaci√≥n de claves de cach√©."""
        text = "Texto de prueba"
        lang = "es"
        max_tokens = 100
        tone = "neutral"
        
        key1 = redis_service._generate_cache_key(text, lang, max_tokens, tone)
        key2 = redis_service._generate_cache_key(text, lang, max_tokens, tone)
        
        # Mismas entradas deben generar la misma clave
        assert key1 == key2
        
        # Diferentes entradas deben generar claves diferentes
        key3 = redis_service._generate_cache_key(text + " modificado", lang, max_tokens, tone)
        assert key1 != key3
```

## Pruebas de Rendimiento

### Pruebas de Carga

```python
# tests/performance/test_load.py
import pytest
import asyncio
import time
from concurrent.futures import ThreadPoolExecutor

from app.main import app
from fastapi.testclient import TestClient

@pytest.mark.performance
@pytest.mark.slow
class TestLoadPerformance:
    @pytest.fixture
    def client(self):
        return TestClient(app)

    def test_concurrent_requests(self, client, sample_text, api_key):
        """Probar m√∫ltiples solicitudes concurrentes."""
        def make_request():
            response = client.post(
                "/v1/summarize",
                headers={"Authorization": f"Bearer {api_key}"},
                json={"text": sample_text}
            )
            return response.status_code, response.elapsed.total_seconds()

        # Ejecutar 20 solicitudes concurrentes
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(make_request) for _ in range(20)]
            results = [future.result() for future in futures]

        # Verificar que todas las solicitudes fueron exitosas
        status_codes = [result[0] for result in results]
        response_times = [result[1] for result in results]

        # Al menos el 90% deber√≠a ser exitoso (200 o 429 por l√≠mite de velocidad)
        successful_requests = sum(1 for code in status_codes if code in [200, 429])
        assert successful_requests >= len(status_codes) * 0.9

        # El tiempo de respuesta promedio deber√≠a ser menor a 5 segundos
        avg_response_time = sum(response_times) / len(response_times)
        assert avg_response_time < 5.0

    def test_response_time_percentiles(self, client, sample_text, api_key):
        """Probar percentiles de tiempo de respuesta."""
        response_times = []
        
        # Hacer 100 solicitudes
        for _ in range(100):
            start_time = time.time()
            response = client.post(
                "/v1/summarize",
                headers={"Authorization": f"Bearer {api_key}"},
                json={"text": sample_text}
            )
            end_time = time.time()
            
            if response.status_code == 200:
                response_times.append(end_time - start_time)

        response_times.sort()
        
        # Calcular percentiles
        p50 = response_times[int(len(response_times) * 0.5)]
        p95 = response_times[int(len(response_times) * 0.95)]
        p99 = response_times[int(len(response_times) * 0.99)]

        # Verificar que los percentiles est√°n dentro de l√≠mites aceptables
        assert p50 < 2.0, f"P50 demasiado alto: {p50}"
        assert p95 < 5.0, f"P95 demasiado alto: {p95}"
        assert p99 < 10.0, f"P99 demasiado alto: {p99}"

    def test_memory_usage(self, client, sample_text, api_key):
        """Probar uso de memoria durante carga."""
        import psutil
        import os

        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB

        # Hacer m√∫ltiples solicitudes
        for _ in range(50):
            response = client.post(
                "/v1/summarize",
                headers={"Authorization": f"Bearer {api_key}"},
                json={"text": sample_text}
            )

        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory

        # El aumento de memoria deber√≠a ser menor a 100MB
        assert memory_increase < 100, f"Aumento de memoria excesivo: {memory_increase}MB"
```

## Pruebas de Seguridad

### Pruebas de Autenticaci√≥n

```python
# tests/security/test_auth.py
import pytest
from fastapi.testclient import TestClient

from app.main import app

@pytest.mark.security
class TestAuthentication:
    @pytest.fixture
    def client(self):
        return TestClient(app)

    def test_missing_authorization_header(self, client, sample_text):
        """Probar solicitud sin header de autorizaci√≥n."""
        response = client.post(
            "/v1/summarize",
            json={"text": sample_text}
        )
        
        assert response.status_code == 401
        data = response.json()
        assert "error" in data
        assert "authentication" in data["error"].lower()

    def test_invalid_authorization_scheme(self, client, sample_text):
        """Probar esquema de autorizaci√≥n inv√°lido."""
        response = client.post(
            "/v1/summarize",
            headers={"Authorization": "Basic invalid_token"},
            json={"text": sample_text}
        )
        
        assert response.status_code == 401

    def test_invalid_api_key(self, client, sample_text):
        """Probar clave API inv√°lida."""
        response = client.post(
            "/v1/summarize",
            headers={"Authorization": "Bearer invalid_key_12345"},
            json={"text": sample_text}
        )
        
        assert response.status_code == 401

    def test_empty_api_key(self, client, sample_text):
        """Probar clave API vac√≠a."""
        response = client.post(
            "/v1/summarize",
            headers={"Authorization": "Bearer "},
            json={"text": sample_text}
        )
        
        assert response.status_code == 401

    def test_api_key_injection_attempt(self, client, sample_text):
        """Probar intento de inyecci√≥n en clave API."""
        malicious_key = "'; DROP TABLE users; --"
        
        response = client.post(
            "/v1/summarize",
            headers={"Authorization": f"Bearer {malicious_key}"},
            json={"text": sample_text}
        )
        
        assert response.status_code == 401

    def test_rate_limiting_by_api_key(self, client, sample_text):
        """Probar que la limitaci√≥n de velocidad es por clave API."""
        api_key_1 = "test_key_1"
        api_key_2 = "test_key_2"
        
        # Agotar l√≠mite para primera clave
        for _ in range(10):
            response = client.post(
                "/v1/summarize",
                headers={"Authorization": f"Bearer {api_key_1}"},
                json={"text": sample_text}
            )
        
        # Verificar que la primera clave est√° limitada
        assert response.status_code == 429
        
        # La segunda clave deber√≠a funcionar normalmente
        response = client.post(
            "/v1/summarize",
            headers={"Authorization": f"Bearer {api_key_2}"},
            json={"text": sample_text}
        )
        
        # Deber√≠a ser exitosa o al menos no limitada por velocidad
        assert response.status_code != 429
```

### Pruebas de Validaci√≥n de Entrada

```python
# tests/security/test_input.py
import pytest
from fastapi.testclient import TestClient

from app.main import app

@pytest.mark.security
class TestInputValidation:
    @pytest.fixture
    def client(self):
        return TestClient(app)

    def test_sql_injection_in_text(self, client, api_key):
        """Probar intento de inyecci√≥n SQL en texto."""
        malicious_text = "'; DROP TABLE users; SELECT * FROM passwords WHERE '1'='1"
        
        response = client.post(
            "/v1/summarize",
            headers={"Authorization": f"Bearer {api_key}"},
            json={"text": malicious_text}
        )
        
        # Deber√≠a ser rechazado por validaci√≥n o procesado de forma segura
        assert response.status_code in [200, 400]

    def test_xss_in_text(self, client, api_key):
        """Probar intento de XSS en texto."""
        malicious_text = "<script>alert('XSS')</script>Texto normal"
        
        response = client.post(
            "/v1/summarize",
            headers={"Authorization": f"Bearer {api_key}"},
            json={"text": malicious_text}
        )
        
        if response.status_code == 200:
            data = response.json()
            # El script deber√≠a ser escapado o removido
            assert "<script>" not in data["summary"]

    def test_oversized_payload(self, client, api_key):
        """Probar payload excesivamente grande."""
        oversized_text = "a" * 100000  # 100KB de texto
        
        response = client.post(
            "/v1/summarize",
            headers={"Authorization": f"Bearer {api_key}"},
            json={"text": oversized_text}
        )
        
        assert response.status_code == 400

    def test_special_characters(self, client, api_key):
        """Probar caracteres especiales en entrada."""
        special_text = "Texto con emojis üöÄ y caracteres especiales: √±√°√©√≠√≥√∫"
        
        response = client.post(
            "/v1/summarize",
            headers={"Authorization": f"Bearer {api_key}"},
            json={"text": special_text}
        )
        
        # Deber√≠a manejar caracteres especiales correctamente
        assert response.status_code in [200, 400]

    def test_null_bytes(self, client, api_key):
        """Probar bytes nulos en entrada."""
        text_with_nulls = "Texto\0con\0bytes\0nulos"
        
        response = client.post(
            "/v1/summarize",
            headers={"Authorization": f"Bearer {api_key}"},
            json={"text": text_with_nulls}
        )
        
        # Deber√≠a rechazar o limpiar bytes nulos
        assert response.status_code in [200, 400]
```

## Ejecuci√≥n de Pruebas

### Comandos B√°sicos

```bash
# Ejecutar todas las pruebas
pytest

# Ejecutar solo pruebas unitarias
pytest -m unit

# Ejecutar solo pruebas de integraci√≥n
pytest -m integration

# Ejecutar pruebas con cobertura
pytest --cov=app --cov-report=html

# Ejecutar pruebas en paralelo
pytest -n auto

# Ejecutar pruebas espec√≠ficas
pytest tests/unit/test_config.py::TestConfig::test_default_values
```

### Comandos Avanzados

```bash
# Ejecutar pruebas con diferentes niveles de verbosidad
pytest -v                    # Verboso
pytest -vv                   # Muy verboso
pytest -q                    # Silencioso

# Ejecutar pruebas que fallaron en la √∫ltima ejecuci√≥n
pytest --lf

# Ejecutar pruebas hasta el primer fallo
pytest -x

# Ejecutar pruebas con timeout
pytest --timeout=300

# Generar reporte JUnit para CI/CD
pytest --junitxml=report.xml
```

### Configuraci√≥n de CI/CD

```yaml
# .github/workflows/test.yml
name: Pruebas

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Configurar Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Instalar dependencias
      run: |
        pip install -r requirements.txt
    
    - name: Ejecutar pruebas unitarias
      run: |
        pytest -m unit --cov=app --cov-report=xml
    
    - name: Ejecutar pruebas de integraci√≥n
      run: |
        pytest -m integration
      env:
        REDIS_URL: redis://localhost:6379/0
    
    - name: Subir cobertura
      uses: codecov/codecov-action@v3
```

## M√©tricas de Calidad

### Cobertura de C√≥digo

**Objetivo:** Mantener cobertura del 80%+

```bash
# Generar reporte de cobertura
pytest --cov=app --cov-report=html --cov-report=term-missing

# Verificar cobertura m√≠nima
pytest --cov=app --cov-fail-under=80
```

### M√©tricas de Pruebas

| M√©trica | Objetivo | Actual |
|---------|----------|--------|
| Cobertura de C√≥digo | > 80% | 85% |
| Pruebas Unitarias | > 200 | 250+ |
| Pruebas de Integraci√≥n | > 50 | 75+ |
| Tiempo de Ejecuci√≥n | < 5 min | 3 min |
| Tasa de √âxito | > 95% | 98% |

### Reportes de Calidad

```bash
# Generar reporte completo de calidad
pytest --cov=app --cov-report=html --cov-report=xml --junitxml=test-results.xml

# An√°lisis de complejidad ciclom√°tica
radon cc app/ -a

# An√°lisis de seguridad
bandit -r app/

# An√°lisis de estilo de c√≥digo
flake8 app/
black --check app/
```

## Mejores Pr√°cticas

### 1. Organizaci√≥n de Pruebas

- **Separar por tipo**: unitarias, integraci√≥n, rendimiento, seguridad
- **Usar fixtures**: Para datos de prueba reutilizables
- **Nombres descriptivos**: Que expliquen qu√© se est√° probando
- **Una aserci√≥n por prueba**: Cuando sea posible

### 2. Datos de Prueba

- **Usar datos realistas**: Pero no datos reales sensibles
- **Cubrir casos l√≠mite**: Valores m√≠nimos, m√°ximos, nulos
- **Datos de prueba variados**: Diferentes idiomas, longitudes, formatos

### 3. Mocking y Stubbing

- **Mock servicios externos**: LLM, Redis, APIs externas
- **Usar AsyncMock**: Para funciones as√≠ncronas
- **Verificar interacciones**: Que los mocks se llamen correctamente

### 4. Pruebas de Rendimiento

- **Establecer l√≠neas base**: Tiempos de respuesta aceptables
- **Probar bajo carga**: M√∫ltiples solicitudes concurrentes
- **Monitorear recursos**: Memoria, CPU, conexiones de red

### 5. Pruebas de Seguridad

- **Validar entrada**: Caracteres especiales, tama√±os, formatos
- **Probar autenticaci√≥n**: Claves v√°lidas e inv√°lidas
- **Verificar autorizaci√≥n**: Acceso a recursos protegidos